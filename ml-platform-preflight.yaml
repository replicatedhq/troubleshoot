apiVersion: troubleshoot.sh/v1beta3
kind: Preflight
metadata:
  name: ml-platform-requirements
requirements:
  - name: GPU Infrastructure
    docString: |
      Title: GPU and Accelerator Requirements
      Requirement:
        - GPU Nodes: Minimum {{ .gpu.minNodes | default "2" }} nodes with NVIDIA GPUs
        - GPU Model: {{ .gpu.model | default "NVIDIA Tesla T4" }} or newer
        - CUDA Version: >= {{ .gpu.cudaVersion | default "11.8" }}
        - GPU Memory per card: Minimum {{ .gpu.minMemory | default "16" }}GB
        - NVIDIA GPU Operator: Required for automatic driver management
        - Device Plugin: nvidia-device-plugin-daemonset deployed
    checks:
      - nodeResources:
          checkName: GPU node availability
          filters:
            selector:
              matchLabels:
                nvidia.com/gpu: "true"
          outcomes:
            - fail:
                when: 'count() < {{ .gpu.minNodes | default "2" }}'
                message: Requires at least {{ .gpu.minNodes | default "2" }} GPU nodes
            - pass:
                message: Sufficient GPU nodes available
      - customResourceDefinition:
          checkName: NVIDIA GPU Operator CRDs
          customResourceDefinitionName: clusterpolicies.nvidia.com
          outcomes:
            - warn:
                message: NVIDIA GPU Operator CRDs not found - manual GPU management required
            - pass:
                message: NVIDIA GPU Operator detected

  - name: ML Frameworks and Operators
    docString: |
      Title: Machine Learning Framework Requirements
      Requirement:
        - Kubeflow Operator: {{ if .kubeflow.enabled }}Required{{ else }}Optional{{ end }}
        - PyTorch Operator: {{ if .pytorch.enabled }}Required{{ else }}Optional{{ end }}
        - TensorFlow Operator: {{ if .tensorflow.enabled }}Required{{ else }}Optional{{ end }}
        - MLflow tracking server: {{ .Values.mlflow.endpoint | default "http://mlflow.default.svc.cluster.local:5000" }}
        - Model registry: {{ .Values.registry.type | default "Harbor" }} with OCI artifact support
    checks:
      {{- if .Values.kubeflow.enabled }}
      - customResourceDefinition:
          checkName: Kubeflow Training Operator
          customResourceDefinitionName: pytorchjobs.kubeflow.org
          outcomes:
            - fail:
                message: Kubeflow Training Operator required but not installed
            - pass:
                message: Kubeflow Training Operator available
      {{- end }}
      {{- if .Values.pytorch.enabled }}
      - customResourceDefinition:
          checkName: PyTorch Job CRD
          customResourceDefinitionName: pytorchjobs.pytorch.org
          outcomes:
            - fail:
                message: PyTorch operator required but not found
            - pass:
                message: PyTorch operator installed
      {{- end }}

  - name: High-Performance Storage
    docString: |
      Title: Storage Requirements for ML Workloads
      Requirement:
        - Dataset Storage Class: {{ .storage.datasetClass | default "fast-nvme" }}
        - Model Storage Class: {{ .storage.modelClass | default "replicated-ssd" }}
        - Shared filesystem: {{ .storage.sharedFS | default "NFS" }} or CSI driver with RWX
        - Minimum throughput: {{ .storage.minThroughput | default "500" }}MB/s sequential read
        - IOPS requirement: {{ .storage.minIOPS | default "10000" }} for training data
        - S3-compatible object storage: {{ if .s3.enabled }}Required at {{ .s3.endpoint }}{{ else }}Optional{{ end }}
    checks:
      - storageClass:
          checkName: Dataset storage class
          name: '{{ .storage.datasetClass | default "fast-nvme" }}'
          required: true
      - storageClass:
          checkName: Model storage class  
          name: '{{ .storage.modelClass | default "replicated-ssd" }}'
          required: true
      {{- if .s3.enabled }}
      - secret:
          checkName: S3 credentials
          secretName: '{{ .s3.secretName | default "s3-credentials" }}'
          namespace: '{{ .s3.namespace | default "default" }}'
          key: '{{ .s3.accessKeyField | default "access-key" }}'
          outcomes:
            - fail:
                message: S3 credentials secret not found
            - pass:
                message: S3 credentials configured
      {{- end }}

  - name: Network Performance
    docString: |
      Title: Network Requirements for Distributed Training
      Requirement:
        - CNI Plugin: {{ .Values.network.cni | default "Calico" }} with RDMA support preferred
        - Network bandwidth: Minimum {{ .Values.network.minBandwidth | default "10" }}Gbps between nodes
        - InfiniBand: {{ if .Values.network.infiniband }}Required{{ else }}Optional for best performance{{ end }}
        - Service mesh: {{ if .Values.network.istio }}Istio with telemetry disabled for training jobs{{ else }}Not required{{ end }}
        - Load balancer: Required for model serving endpoints
    checks:
      - clusterPods:
          namespaces:
            - kube-system
          selector:
            matchLabels:
              k8s-app: '{{ .Values.network.cni | default "calico" }}-node'
          outcomes:
            - fail:
                when: "count() == 0"
                message: CNI plugin {{ .network.cni | default "Calico" }} not detected
            - pass:
                message: CNI plugin operational
      {{- if .Values.network.istio }}
      - namespace:
          checkName: Istio system namespace
          namespace: istio-system
          outcomes:
            - fail:
                message: Istio namespace not found
            - pass:
                message: Istio detected
      {{- end }}

  {{- if .Values.jupyter.enabled }}
  - name: JupyterHub Requirements
    docString: |
      Title: JupyterHub Platform Requirements
      Requirement:
        - JupyterHub version: >= {{ .jupyter.version | default "3.0" }}
        - User storage: {{ .jupyter.storagePerUser | default "100Gi" }} per user workspace
        - Default image: {{ .jupyter.defaultImage | default "jupyter/tensorflow-notebook:latest" }}
        - GPU passthrough: {{ if .jupyter.gpuEnabled }}Enabled with scheduler hints{{ else }}Disabled{{ end }}
        - OAuth provider: {{ .jupyter.authProvider | default "GitHub" }}
    checks:
      - deployment:
          checkName: JupyterHub deployment
          name: jupyterhub
          namespace: '{{ .Values.jupyter.namespace | default "jupyterhub" }}'
          outcomes:
            - fail:
                message: JupyterHub deployment not found
            - pass:
                message: JupyterHub is deployed
      - configMap:
          checkName: JupyterHub configuration
          configMapName: jupyterhub-config
          namespace: '{{ .Values.jupyter.namespace | default "jupyterhub" }}'
          key: jupyterhub_config.py
          outcomes:
            - fail:
                message: JupyterHub configuration missing
            - pass:
                message: JupyterHub configured
  {{- end }}

  - name: Compute Resource Pools
    docString: |
      Title: Node Pool Requirements for ML Workloads
      Requirement:
        - CPU-only pool: Minimum {{ .Values.pools.cpu.minNodes | default "3" }} nodes for services
        - GPU pool: Minimum {{ .Values.pools.gpu.minNodes | default "2" }} nodes for training
        - Spot/Preemptible pool: {{ if .Values.pools.spot.enabled }}{{ .Values.pools.spot.minNodes | default "5" }} nodes for batch jobs{{ else }}Not required{{ end }}
        - Node labels:
          - workload-type: cpu-compute | gpu-compute | spot-compute
          - node-lifecycle: on-demand | spot
    checks:
      - nodeResources:
          checkName: CPU compute pool
          filters:
            selector:
              matchLabels:
                workload-type: cpu-compute
          outcomes:
            - fail:
                when: 'count() < {{ .Values.pools.cpu.minNodes | default "3" }}'
                message: Requires at least {{ .Values.pools.cpu.minNodes | default "3" }} CPU compute nodes
            - pass:
                message: CPU compute pool sufficient
      {{- if .Values.pools.spot.enabled }}
      - nodeResources:
          checkName: Spot instance pool
          filters:
            selector:
              matchLabels:
                node-lifecycle: spot
          outcomes:
            - warn:
                when: 'count() < {{ .Values.pools.spot.minNodes | default "5" }}'
                message: Spot pool has fewer than recommended nodes
            - pass:
                message: Spot instance pool available
      {{- end }}

  {{- if .Values.monitoring.enabled }}
  - name: ML Monitoring and Metrics
    docString: |
      Title: ML-Specific Monitoring Requirements
      Requirement:
        - Metrics backend: {{ .Values.monitoring.backend | default "Prometheus + VictoriaMetrics" }}
        - GPU metrics: dcgm-exporter required
        - Training metrics: TensorBoard server deployment
        - Model metrics: {{ .monitoring.modelMetrics | default "Seldon Core Analytics" }}
        - Experiment tracking: {{ .monitoring.experiments | default "MLflow" }}
        - Data drift detection: {{ if .monitoring.dataDrift }}Enabled{{ else }}Optional{{ end }}
    checks:
      - daemonSet:
          checkName: DCGM Exporter for GPU metrics
          name: dcgm-exporter
          namespace: '{{ .Values.monitoring.namespace | default "gpu-operator-resources" }}'
          outcomes:
            - warn:
                message: DCGM exporter not found - GPU metrics unavailable
            - pass:
                message: GPU metrics collection enabled
      - deployment:
          checkName: TensorBoard server
          name: tensorboard
          namespace: '{{ .Values.monitoring.namespace | default "kubeflow" }}'
          outcomes:
            - warn:
                message: TensorBoard not deployed
            - pass:
                message: TensorBoard available for training visualization
  {{- end }}

  - name: Security and Compliance
    docString: |
      Title: ML Platform Security Requirements
      Requirement:
        - Pod Security Standards: {{ .Values.security.podSecurityLevel | default "restricted" }} enforced
        - Image scanning: {{ if .Values.security.scanning }}Required with Trivy or Snyk{{ else }}Recommended{{ end }}
        - Secrets management: {{ .Values.security.secretsManager | default "Sealed Secrets" }} or external KMS
        - Network policies: Required for namespace isolation
        - Audit logging: {{ if .Values.security.auditLog }}Enabled with model access tracking{{ else }}Optional{{ end }}
        - Data encryption: At-rest and in-transit required
    checks:
      - namespace:
          checkName: Pod Security Standards
          namespace: '{{ .Values.targetNamespace | default "ml-platform" }}'
          outcomes:
            - fail:
                message: Target namespace does not exist
            - pass:
                message: Target namespace configured
      {{- if .Values.security.scanning }}
      - customResourceDefinition:
          checkName: Trivy Operator
          customResourceDefinitionName: vulnerabilityreports.aquasecurity.github.io
          outcomes:
            - fail:
                message: Image scanning required but Trivy operator not found
            - pass:
                message: Image vulnerability scanning enabled
      {{- end }}

  {{- if .Values.backup.enabled }}
  - name: Backup and Disaster Recovery
    docString: |
      Title: Backup Requirements for ML Assets
      Requirement:
        - Backup solution: {{ .Values.backup.solution | default "Velero" }}
        - Model snapshots: Daily backups with {{ .Values.backup.modelRetention | default "30" }} day retention
        - Dataset backups: {{ if .Values.backup.datasets }}Incremental backups to object storage{{ else }}Not required{{ end }}
        - Experiment tracking DB: Point-in-time recovery required
        - Backup storage: {{ .Values.backup.storageLocation | default "s3://ml-backups" }}
    checks:
      - deployment:
          checkName: Velero backup controller
          name: velero
          namespace: velero
          outcomes:
            - fail:
                message: Velero not installed - backup capability missing
            - pass:
                message: Backup solution operational
      - customResourceDefinition:
          checkName: Backup CRDs
          customResourceDefinitionName: backups.velero.io
          outcomes:
            - fail:
                message: Velero CRDs missing
            - pass:
                message: Backup resources available
  {{- end }}

  - name: Autoscaling and Elasticity
    docString: |
      Title: Autoscaling Requirements for ML Workloads
      Requirement:
        - Cluster Autoscaler: Required with {{ .Values.autoscaling.provider | default "AWS" }} provider
        - HPA: Required for inference services
        - VPA: {{ if .Values.autoscaling.vpa }}Enabled for training jobs{{ else }}Optional{{ end }}
        - KEDA: {{ if .Values.autoscaling.keda }}Required for event-driven scaling{{ else }}Optional{{ end }}
        - Scale-down delay: {{ .Values.autoscaling.scaleDownDelay | default "10m" }} for GPU nodes
        - Max cluster size: {{ .Values.autoscaling.maxNodes | default "100" }} nodes
    checks:
      - deployment:
          checkName: Cluster Autoscaler
          name: cluster-autoscaler
          namespace: kube-system
          outcomes:
            - fail:
                message: Cluster autoscaler required but not found
            - pass:
                message: Cluster autoscaler operational
      {{- if .Values.autoscaling.keda }}
      - customResourceDefinition:
          checkName: KEDA ScaledObject
          customResourceDefinitionName: scaledobjects.keda.sh
          outcomes:
            - fail:
                message: KEDA required but not installed
            - pass:
                message: KEDA event-driven autoscaling available
      {{- end }}